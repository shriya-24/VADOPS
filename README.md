# VADOPS - Verification and Analysis of Augmented Data generated by various prompting strategies using off-the-shelf LLMs
Repository to contain the code for 696DS project (Spr 2023)

## Folder Structure:
```text
.
├── README.md
├── checkpoints
├── data
│   ├── README.md
│   └── snips
│       ├── test.csv
│       ├── train.csv
│       └── validation.csv
├── models
│   ├── CLINC.py
│   ├── SNIPS.py
│   └── main.py
│   └── workflow.py
├── notebooks
│   ├── BERT.ipynb
│   ├── README.md
│   ├── eval_clinc.ipynb
│   └── finetune_clinc.ipynb
├── output
├── predictions
├── training_dynamics
├── datamap_graphs
└── workflow_configs
└── scripts
    ├── README.md
    ├── eval_clinc.sh
    ├── eval_snips.sh
    ├── finetune_clinc.sh
    ├── finetune_snips.sh
    └── logs
```

## About Folders:
  - data:  contain subfolders where folder_name = dataset_name. Currently, it only contains snips subfolder which has train, validation, test files in csv format.
  - models:contains PyTorch code 
  - notebooks: contains jupyter notebooks 
  - scripts: contains finetuning and evaluation scripts for datasets. Also contains `logs` subfolder(to save the slurm output)
  - checkpoints: to save finetuned model checkpoints
  - predictions: to save prediction results of the checkpoint model passed
  - training_dynamics: to save the training_dynamics during the finetuning
  - datamap_graphs: this is where we save the datamap graphs for a model using the training_dynamics
  - workflow_configs: contains all the workflow configuration
  - output: used to saved the output for the workflows

**Note:** One can choose different directory path for saving finetuned model checkpoints, prediction and slurm outputs. But, I highly recommend to follow the above folder structure as we gonna work on each workspaces in future.

## Quick overview people should be aware of pipeline

## Pseudo code for pipeline:
    Loop(n steps):
    1. fine-tune the model
    2. eval the model
    3. generate data using LLM(chatGPT)
    4. augment the data to train data

1. Config file - this is JSON file which has to be passed to run the workflow. The keys in JSON file is as follows
    1. model_name_or_path - specify the model name or checkpoint path
    2. training arguments - which you need for the model
    3. dataset_name - specify the dataset name as [“clinc_oos”, “CSAbstruct”, “snips”]
    4. dataset_subset - if there any subset you wanna load. For CLINC - [“imbalanced”, “plus”, “small”]
    5. workflow_output_dir - this is where the output for each step gets saved
    6. steps - how many times the pipeline should run
    7. dynamics - specify the dataset types for which dataset cartography has to be plotted
    8. eval - specify dataset types on which evaluation should be measured(Intent class analysis and {Acc, macro_F1 score, weighted_F1 score as whole}
    9. entropy -  specify dataset types on which entropy loss for each sentence is evaluated
    10. generate_data_from - specify the dataset type to refer for data generation

### Note: All the config.json files are stored in the workflow_configs folder.

2. All the pipeline code is available in workflow.py

3. When you run the pipeline, the output of the workflow is saved in the specified workflow_output_dir in the config.json
    1. A folder is created with name using the time when the workflow is called and it contains subfolders with name specifying each step(0, 1, 2) and config file is copied which is used by workflow
    2. Each sub folder has
        1. dynamics folder- the dynamics for each dataset type saved
        2. entropy folder- contains csv file with the dataset type name. The csv file contains sentence and their entropy loss
        3. intent_analysis folder-  contains csv file with the dataset type name. The csv file contains Acc, macro_F1 score, weighted_F1 score for each intent class
        4. model folder- this is where checkpoints are saved
        5. dataMaps folder- this is where dataMaps are saved
        6. generated_data.csv - this is data generated by LLM

4. sbatch for the pipeline is created. All we need to pass the config.json file_path to the command ```sbatch workflow.sh /path/to/workflow_config_json_file```


### Note: In order to handle OpenAI RateLimitError or when the OpenAI is overloaded, we are sleeping the function for 60 seconds.

## Script Commands:
  Before running any of the following command, change your working directory to scripts folder
  - For Finetuning:

    - CLINC:

      ```sbatch finetune_clinc.sh {clinc_subset} /path/to/checkpoint```

    - SNIPS:

      ```sbatch finetune_snips.sh /path/to/checkpoint```

  - For Finetuning along with Dataset Cartography:

    - CLINC:
        ```sbatch finetune_clinc_cartography.sh {clinc_subset} /path/to/checkpoint {cartography_split} {log_dynamics_dir}```

    - SNIPS:

      ```sbatch finetune_snips_cartography.sh /path/to/checkpoint {cartography_splits} {log_dynamics_dir}```


  - For Evaluation:
    
    - CLINC:

      ```sbatch eval_clinc.sh {clinc_subset} {dataset_type} /path/to/checkpoint /path/to/prediction/{fileName}.csv```

    - SNIPS:

      ```sbatch eval_snips.sh {dataset_type} /path/to/checkpoint /path/to/prediction/{fileName}.csv```

  - For Calculating Entropy Loss:
    
    - CLINC:

      ```sbatch calc_entropy_loss_clinc.sh {clinc_subset} {dataset_type} /path/to/checkpoint /path/to/entropy_analysis_dir/{fileName}.csv```

    - SNIPS:

      ```sbatch calc_entropy_loss_snips.sh {dataset_type} /path/to/checkpoint /path/to/entropy_analysis_dir/{fileName}.csv```
  
  - For Plotting DataMap for a saved dynamics

     ```sbatch plot_dataMap.sh /path/to/log_dynamics_dir  /path/to/plot_dir {dataset_name}```

  - Run Workflow
  
    ```sbatch workflow.sh /path/to/workflow_config_json_file```

    ***Note:*** For more details about the scripts, checkout the readme file in the scripts folder.


### TODO: Currently this file, is used to understand the folder structure and what are the script commands. Will be updated once everyone got familiar with the above.
